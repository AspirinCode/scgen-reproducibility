{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanpy==1.4 anndata==0.6.18 numpy==1.15.4 scipy==1.2.0 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy.api as sc\n",
    "import anndata\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "sc.settings.verbosity = 3  # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.logging.print_versions()\n",
    "sc.settings.set_figure_params(dpi=80)  # low dpi (dots per inch) yields small inline figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered out 376 cells that have less than 600 genes expressed\n"
     ]
    }
   ],
   "source": [
    "train_path = \"../data/pancreas.h5ad\"\n",
    "if os.path.isfile(train_path):\n",
    "    adata = sc.read(train_path)\n",
    "else:\n",
    "    train_url = \"https://www.dropbox.com/s/qj1jlm9w10wmt0u/pancreas.h5ad?dl=1\"\n",
    "    t_dl = wget.download(train_url, train_path)\n",
    "    adata = sc.read(train_path)\n",
    "adata = anndata.AnnData(X=np.expm1(adata.raw.X), var=adata.raw.var, obs=adata.obs)\n",
    "sc.pp.filter_cells(adata, min_genes=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempts to run Scanorama within a Jupyter Notebook have been unsuccessful. However, exporting the data outside of the notebook and running Scanorama's entire workflow, including turning the data into `.npz` pickles results in functional output. So let's export the rawest form of the count matrices for Scanorama to process.\n",
    "\n",
    "This notebook won't run immediately on your system as you need a Scanorama GitHub clone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(data=adata[adata.obs['sample']=='Baron'].X.todense().transpose(),\n",
    "                  index=adata[adata.obs['sample']=='Baron'].var_names,\n",
    "                  columns=np.arange(np.sum(adata.obs['sample']=='Baron')))\n",
    "\n",
    "df2 = pd.DataFrame(data=adata[adata.obs['sample']=='Muraro'].X.todense().transpose(),\n",
    "                  index=adata[adata.obs['sample']=='Muraro'].var_names,\n",
    "                  columns=np.arange(np.sum(adata.obs['sample']=='Muraro')))\n",
    "\n",
    "df3 = pd.DataFrame(data=adata[adata.obs['sample']=='Segerstolpe'].X.todense().transpose(),\n",
    "                  index=adata[adata.obs['sample']=='Segerstolpe'].var_names,\n",
    "                  columns=np.arange(np.sum(adata.obs['sample']=='Segerstolpe')))\n",
    "\n",
    "df4 = pd.DataFrame(data=adata[adata.obs['sample']=='Wang'].X.todense().transpose(),\n",
    "                  index=adata[adata.obs['sample']=='Wang'].var_names,\n",
    "                  columns=np.arange(np.sum(adata.obs['sample']=='Wang')))\n",
    "\n",
    "if not os.path.exists('../../scanorama/data/4panc'):\n",
    "    os.makedirs('../../scanorama/data/4panc')\n",
    "df1.to_csv('../../scanorama/data/4panc/baron.txt',sep='\\t')\n",
    "df2.to_csv('../../scanorama/data/4panc/muraro.txt',sep='\\t')\n",
    "df3.to_csv('../../scanorama/data/4panc/segerstolpe.txt',sep='\\t')\n",
    "df4.to_csv('../../scanorama/data/4panc/wang.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Scanorama. To do this, you need to create this file at `conf/4panc.txt`:\n",
    "\n",
    "\tdata/4panc/baron\n",
    "\tdata/4panc/muraro\n",
    "\tdata/4panc/segerstolpe\n",
    "\tdata/4panc/wang\n",
    "\n",
    "...and this file at `bin/4panc.py`:\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import normalize, LabelEncoder\n",
    "    import sys\n",
    "\n",
    "    from process import load_names, merge_datasets, save_datasets\n",
    "    from scanorama import correct, visualize, process_data\n",
    "    from scanorama import dimensionality_reduce\n",
    "\n",
    "    import time\n",
    "    from datetime import timedelta\n",
    "\n",
    "    data_names = [\n",
    "        'data/4panc/baron',\n",
    "        'data/4panc/muraro',\n",
    "        'data/4panc/segerstolpe',\n",
    "        'data/4panc/wang'\n",
    "    ]\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        datasets, genes_list, n_cells = load_names(data_names)\n",
    "        t1 = time.time()\n",
    "        datasets, genes = correct(datasets, genes_list)\n",
    "        datasets = [ normalize(ds, axis=1) for ds in datasets ]\n",
    "        t2 = time.time()\n",
    "        print('Took '+str(timedelta(seconds=t2-t1)))\n",
    "\n",
    "        save_datasets(datasets, genes, data_names)\n",
    "\n",
    "As you can see, we have timing baked into the script, only measuring the time required to perform the actual batch correction. We export the resulting expression profiles so that we can analyse them within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Loaded data/4panc/baron with 24516 genes and 8569 cells', 'Loaded data/4panc/muraro with 24516 genes and 2126 cells', 'Loaded data/4panc/segerstolpe with 24516 genes and 2987 cells', 'Loaded data/4panc/wang with 24516 genes and 635 cells', 'Found 14317 cells among all datasets', 'Found 24516 genes among all datasets', '[[0.         0.12699906 0.37261466 0.08503937]', ' [0.         0.         0.38523048 0.0976378 ]', ' [0.         0.         0.         0.82047244]', ' [0.         0.         0.         0.        ]]', 'Processing datasets (2, 3)', 'Processing datasets (1, 2)', 'Processing datasets (0, 2)', 'Processing datasets (0, 1)', 'Took 0:02:27.044554', '']\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../../scanorama/')\n",
    "\n",
    "subprocess.run('python bin/process.py conf/4panc.txt', shell=True)\n",
    "res = subprocess.run('python bin/4panc.py', shell=True, stdout=subprocess.PIPE)\n",
    "print(res.stdout.decode('utf-8').split('\\n'))\n",
    "\n",
    "os.chdir('../bbknn/examples/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line in the standard output dump above captures a run time of two minutes. Now that that's done, import the expression back into the notebook and make a new object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/scgen/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/envs/scgen/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \n",
      "/anaconda3/envs/scgen/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/envs/scgen/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "sc1 = pd.read_table('../../scanorama/data/4panc/baron.scanorama_corrected.txt',index_col=0)\n",
    "sc2 = pd.read_table('../../scanorama/data/4panc/muraro.scanorama_corrected.txt',index_col=0)\n",
    "sc3 = pd.read_table('../../scanorama/data/4panc/segerstolpe.scanorama_corrected.txt',index_col=0)\n",
    "sc4 = pd.read_table('../../scanorama/data/4panc/wang.scanorama_corrected.txt',index_col=0)\n",
    "\n",
    "adata_scanorama = anndata.AnnData(X=np.vstack((sc1.values.transpose(),sc2.values.transpose(),\n",
    "                                               sc3.values.transpose(),sc4.values.transpose())),\n",
    "                                  obs=adata.obs, var=sc1.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final object export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adata_scanorama.write('objects-pancreas/pancreas_scanorama.h5ad')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
